akka {
  # Log the complete configuration at INFO level when the actor system is started.
  # This is useful when you are uncertain of what configuration is used.
  log-config-on-start = off

  # stdout-loglevel = "OFF"
  stdout-loglevel = "DEBUG"
  # loglevel = "OFF"
  # loglevel = "DEBUG"
  loglevel = "INFO"
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  log-dead-letters = on
  log-dead-letters-during-shutdown = off

  actor {
    provider = "cluster"
  }


  cluster {
    # Time margin after which shards or singletons that belonged to a downed/removed
    # partition are created in surviving partition. The purpose of this margin is that
    # in case of a network partition the persistent actors in the non-surviving partitions
    # must be stopped before corresponding persistent actors are started somewhere else.
    # This is useful if you implement downing strategies that handle network partitions,
    # e.g. by keeping the larger side of the partition and shutting down the smaller side.
    # It will not add any extra safety for auto-down-unreachable-after, since that is not
    # handling network partitions.
    # Disable with "off" or specify a duration to enable.
    #
    # It is recommended to configure this to the same value as the stable-after property.
    down-removal-margin = 7s

    sharding {
      # Setting for the default shard allocation strategy
      least-shard-allocation-strategy {
        # Threshold of how large the difference between most and least number of
        # allocated shards must be to begin the rebalancing.
        # The difference between number of shards in the region with most shards and
        # the region with least shards must be greater than (>) the `rebalanceThreshold`
        # for the rebalance to occur.
        # 1 gives the best distribution and therefore typically the best choice.
        # Increasing the threshold can result in quicker rebalance but has the
        # drawback of increased difference between number of shards (and therefore load)
        # on different nodes before rebalance will occur.
        # Default is 1 as of Akka 2.5.20
        rebalance-threshold = 1

        # The number of ongoing rebalancing processes is limited to this number.
        # Default is 3 as of Akka 2.5.20
        max-simultaneous-rebalance = 5
      }
    }
  }
}

akka.discovery {
  kubernetes-api {
    pod-label-selector = "app=%s" # same as the default
  }
}

akka.management {
  cluster.bootstrap {
    contact-point-discovery {
      # For the kubernetes API this value is substributed into the %s in pod-label-selector
      service-name = "akka-cluster-demo"

      discovery-method = kubernetes-api
    }
  }
}

# Enable metrics extension in akka-cluster-metrics.
#akka.extensions = ["akka.cluster.metrics.ClusterMetricsExtension"]

# Sigar native library extract location during tests.
# Note: use per-jvm-instance folder when running multiple jvm on one host.
#akka.cluster.metrics.native-library-extract-folder = ${user.dir}/target/native

#akka.cluster.jmx.multi-mbeans-in-same-jvm = on

# Timeout in 15 minutes 15m * 60s = 900s
akka.http.server.idle-timeout = 900s
akka.http.client.idle-timeout = 900s
